# â˜ï¸ COVID-19 Data Pipeline with AWS & Apache Airflow

A fully automated **ETL data pipeline** built using **AWS services** and **Apache Airflow**, designed to extract, process, and analyze COVID-19 data daily.

---

## ğŸš€ Project Overview

This project demonstrates how to build a **real-world cloud data pipeline** leveraging:
- **AWS Lambda** for data extraction
- **Amazon S3** for data storage
- **AWS Athena** for querying raw data
- **Apache Airflow (Docker)** for orchestration and scheduling

The pipeline fetches daily COVID-19 data from a public API, stores it in S3, and makes it queryable using Athena â€” all orchestrated by Airflow.

---

## ğŸ§© Architecture

Public API â†’ AWS Lambda â†’ S3 (Raw Zone) â†’ Athena (Query) â†’ Airflow (Scheduler)

yaml
Copy code

**Workflow:**
1. **Lambda** extracts COVID-19 data via REST API.
2. **Data** is stored in Amazon **S3** as JSON (raw layer).
3. **Athena** uses a JSON SerDe table to query and clean the data.
4. **Airflow** automates this entire pipeline on a daily schedule.

---

## ğŸ› ï¸ Tech Stack

| Layer | Tool/Service | Purpose |
|-------|---------------|----------|
| Orchestration | Apache Airflow (Docker) | Schedule & monitor ETL jobs |
| Compute | AWS Lambda | Serverless data extraction |
| Storage | Amazon S3 | Store raw JSON data |
| Query | AWS Athena | Analyze raw data with SQL |
| Language | Python | Lambda & DAG scripting |
| Notebook | Jupyter | Data analysis and validation |

---

## âš™ï¸ Project Setup

### 1. Clone the Repository
```bash
git clone https://github.com/<your-username>/covid-data-pipeline-aws-airflow.git
cd covid-data-pipeline-aws-airflow
2. Start Airflow with Docker
bash
Copy code
docker-compose up -d
3. Configure AWS
Create an S3 bucket: covid-etl-data-ankit

Update Lambda permissions for s3:PutObject

Set up Athena with database: covid_data

Create JSON table with correct SerDe settings

4. Trigger the DAG
Once Airflow UI is up at http://localhost:8080:

Unpause the DAG: covid_data_pipeline

Run manually or wait for daily schedule

ğŸ“Š Outputs
Raw Data: Stored in S3 (/raw)

Transformed Data: Queried via AWS Athena

Dashboard (optional): Can connect Athena to Power BI for reporting

ğŸ§  Key Learnings
Hands-on experience with AWS Serverless ETL

Built an end-to-end data pipeline

Gained understanding of Airflow orchestration and Athena integration

Learned to handle IAM roles, S3 permissions, and Lambda packaging

ğŸ“š Future Enhancements
Add Power BI dashboard

Integrate AWS Glue for transformations

Deploy Airflow on AWS MWAA

Add monitoring with CloudWatch alerts

ğŸ‘¨â€ğŸ’» Author
Ankit Kumar
Data Engineer @ Accenture | Aspiring Cloud Data Engineer
ğŸ“§ [ankit.your@email.com]
ğŸ”— LinkedIn â€¢ GitHub

â­ If you like this project, consider giving it a star!

yaml
Copy code

---

Would you like me to:
- âœ… customize this README with **your real GitHub + LinkedIn links**,  
- and include a **pipeline diagram image** (which we can generate)?